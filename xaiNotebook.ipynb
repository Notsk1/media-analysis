{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e18888a",
   "metadata": {},
   "source": [
    "# XAI notebook\n",
    "Notebook defines way to test different CNN explainability techniques. This metric used masking and GAN to change the background of the given object in (classification) task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "from utils import map_from_list\n",
    "from torchvision.datasets import VOCDetection, Caltech101, VOCSegmentation, ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ff70c",
   "metadata": {},
   "source": [
    "## Load datasets and pretrained networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f59fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'C:/Users/pette/Documents/jupterNotebooks/machinelearning/datasets' # Own data root directory here\n",
    "choose_dataset = 'Caltech' # VOC, Caltech, ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms for resnet and densenet. Output transform only needed for segmantation:\n",
    "def grey_scale_to_rgb(x):\n",
    "    if x.size(dim=0) == 3:\n",
    "        return x\n",
    "    else:\n",
    "        return x.repeat(3, 1, 1)\n",
    "\n",
    "def image_grey_scale_to_rgb(im):\n",
    "    new = im.convert(mode='RGB')\n",
    "    return new\n",
    "    \n",
    "def VOC_to_label(x):\n",
    "    all_classes = ['horse', 'person', 'bottle', 'dog', 'tvmonitor', 'car', 'aeroplane', 'bicycle',\n",
    "                   'boat', 'chair', 'diningtable', 'pottedplant', 'train', 'cat', 'sofa', 'bird',\n",
    "                   'sheep', 'motorbike', 'bus', 'cow']\n",
    "    final_labels = torch.zeros(len(all_classes))\n",
    "    for one_object in x['annotation']['object']:\n",
    "        final_labels[all_classes.index(one_object['name'])] = 1\n",
    "    return final_labels\n",
    "\n",
    "transform_input = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(grey_scale_to_rgb),\n",
    "    transforms.Normalize(\n",
    "         mean=[0.485, 0.456, 0.406],\n",
    "         std=[0.229, 0.224, 0.225]\n",
    " )\n",
    "])\n",
    "transform_output = transforms.Compose([\n",
    "    transforms.Lambda(VOC_to_label)\n",
    "])\n",
    "\n",
    "caltech_image_transform = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.Lambda(image_grey_scale_to_rgb)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if choose_dataset == 'VOC':\n",
    "    num_of_classes = 20\n",
    "    dataset = VOCDetection(root, year='2012', image_set='train', download=True, transform=transform_input, target_transform=transform_output)\n",
    "    dataset_original = VOCDetection(root, year='2012', image_set='train', download=True, transform=None, target_transform=None)\n",
    "    #dataset_segmentation = VOCSegmentation(root, year='2012', image_set='train', download=True, transform=None, target_transform=transform_output)\n",
    "elif choose_dataset == 'Caltech':\n",
    "    num_of_classes = 101\n",
    "    dataset = Caltech101(root, download=True, transform=transform_input, target_transform=None)\n",
    "    dataset_original = Caltech101(root, download=True, transform=caltech_image_transform, target_transform=None)\n",
    "elif choose_dataset == 'ImageNet':\n",
    "    num_of_classes = 1000\n",
    "    root += '/imagenet_images'\n",
    "    dataset = ImageFolder(root, transform=transform_input, target_transform=None)\n",
    "    dataset_original = ImageFolder(root, transform=None, target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[10])\n",
    "print(dataset_original[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f707cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "model_name = 'ResNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665118cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ResNet':\n",
    "    # Import only if model used\n",
    "    from torchvision.models import resnet34, ResNet34_Weights\n",
    "    \n",
    "    # Loads best possible pre-trained weights for ImageNet dataset (further traning needed for other datasets)\n",
    "    weights = ResNet34_Weights.DEFAULT\n",
    "    # Init model with weights\n",
    "    model = resnet34(weights=weights)\n",
    "    \n",
    "    # Remove global average pooling as it reduces HiResCAM to GradCAM:\n",
    "    model = list(model.children())[:-2]\n",
    "    model = torch.nn.Sequential(*model)\n",
    "    model.add_module(\"flatten\", torch.nn.Flatten())\n",
    "    model.add_module(\"linear_end\", torch.nn.Linear(51200, num_of_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'DenseNet':\n",
    "    # Import only if model used\n",
    "    from torchvision.models import densenet121, DenseNet121_Weights\n",
    "\n",
    "    # Loads best possible pre-trained weights for ImageNet dataset (further traning needed for other datasets)\n",
    "    weights = DenseNet121_Weights.DEFAULT\n",
    "    # Init model with weights\n",
    "    model = densenet121(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4706ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "batch_size = 4\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bdc51",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Model can be further trained with loaded datasets. All classification models pretrained weights are trained on Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "load_weights = True\n",
    "# Create optimizer and loss function\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "momentum = 0\n",
    "lr = 0.001\n",
    "\n",
    "if choose_dataset == 'VOC':\n",
    "    crit = torch.nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    crit = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "\n",
    "# Define number of epochs used for further training\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "                inputs, labels = data[0], data[1]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = crit(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                print(f\"{i}/{len(data_loader)}\")\n",
    "        torch.save(model.state_dict(), f'weights/latest{choose_dataset}.pth')\n",
    "        print(f\"Loss in epoch {epoch}: {running_loss/(len(data_loader)*batch_size)}\")\n",
    "else:\n",
    "    if load_weights:\n",
    "        model.load_state_dict(torch.load(f'weights/latest{choose_dataset}.pth'))\n",
    "    else:\n",
    "        pass # Use pretrained weights for XAI method evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64631f0",
   "metadata": {},
   "source": [
    "# GradCAM example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define couple of fuctions to make code more readable\n",
    "def get_difference_mask(cam_1, cam_2):\n",
    "    # Threshold both:\n",
    "    threshold = 0.3\n",
    "    cam_1_mask = (cam_1 > threshold).astype(int)\n",
    "    cam_2_mask = (cam_2 > threshold).astype(int)\n",
    "    \n",
    "    # Calculate places where different\n",
    "    difference_mask = ((cam_1_mask != cam_2_mask) & (cam_2_mask == 1)).astype(int)\n",
    "    \n",
    "    return difference_mask\n",
    "\n",
    "def mask_input_tensor(input_tensor, difference_mask, random_mask = True):\n",
    "    # Create random mask if defined, otherwise invert the mask\n",
    "    if random_mask:\n",
    "        random_array = np.random.uniform(low = torch.min(input_tensor), high = torch.max(input_tensor),\n",
    "                                         size = (input_tensor.size()))\n",
    "        mask = difference_mask*random_array\n",
    "        mask[mask==0] = 1\n",
    "    else:\n",
    "        mask = 1-difference_mask\n",
    "\n",
    "    output_tensor = torch.mul(input_tensor, torch.tensor(mask))\n",
    "    return output_tensor, mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665f243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(dict(model.named_modules()))\n",
    "\n",
    "num_of_images = 10\n",
    "all_images = random.sample(range(0, len(dataset)), num_of_images)\n",
    "\n",
    "plotting = False\n",
    "new_tensors = {}\n",
    "\n",
    "for number in all_images:\n",
    "    # Get transformed tensor with index\n",
    "    (input_tensor, labels) = dataset[number]\n",
    "\n",
    "\n",
    "    # Get original image with index and reshape(for plotting)\n",
    "    (img, label) = dataset_original[number]\n",
    "    img = cv2.resize(np.array(img), (300, 300))\n",
    "    img = np.float32(img) / 255 # Assume 8 bit pixels\n",
    "\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    if choose_dataset == 'VOC':\n",
    "        labels = (labels==1).nonzero().squeeze().tolist()\n",
    "\n",
    "    if isinstance(labels, int):\n",
    "        labels = [labels]\n",
    "\n",
    "    for label in labels:\n",
    "\n",
    "        output = model(input_tensor)\n",
    "        output = torch.argmax(output)\n",
    "        \n",
    "        # Set target as our predicted label\n",
    "        targets = [ClassifierOutputTarget(output)]\n",
    "        # Define target layer\n",
    "        target_layers = [model[7][2]]\n",
    "\n",
    "        # Run model cams\n",
    "        with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "            grayscale_grad_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "            grad_cam_image = show_cam_on_image(img, grayscale_grad_cams[0, :], use_rgb=True)\n",
    "\n",
    "        with HiResCAM(model=model, target_layers=target_layers) as cam:\n",
    "            grayscale_hires_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "            hires_cam_image = show_cam_on_image(img, grayscale_hires_cams[0, :], use_rgb=True)\n",
    "\n",
    "        if plotting:\n",
    "            # Make images the same format and plot original, greyscale and heatmap:\n",
    "            grad_cam = np.uint8(255*grayscale_grad_cams[0, :])\n",
    "            grad_cam = cv2.merge([grad_cam, grad_cam, grad_cam])\n",
    "            \n",
    "            hires_cam = np.uint8(255*grayscale_hires_cams[0, :])\n",
    "            hires_cam = cv2.merge([hires_cam, hires_cam, hires_cam])\n",
    "            \n",
    "            images = np.hstack((np.uint8(255*img), grad_cam, grad_cam_image, hires_cam, hires_cam_image))\n",
    "            Image.fromarray(images).show()\n",
    "\n",
    "        difference_mask = get_difference_mask(grayscale_grad_cams, grayscale_hires_cams)\n",
    "\n",
    "        masked_tensor, mask = mask_input_tensor(input_tensor, difference_mask, random_mask = False)\n",
    "        # Store prediction and new tensor:\n",
    "        new_tensors[number] = (masked_tensor, output, label, mask)\n",
    "\n",
    "        # Break out of the loop as only one prediction needed\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7305d",
   "metadata": {},
   "source": [
    "## Store masked input image (and load the GAN results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_input_image(input_image, mask):\n",
    "    transform_image = transforms.Compose([\n",
    "        transforms.Resize((300,300)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    mask = mask.squeeze(0)\n",
    "    output_image = torch.mul(transform_image(input_image), torch.tensor(mask))\n",
    "    output_image\n",
    "    transform = transforms.ToPILImage()\n",
    "    return transform(output_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9843bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_pic_and_mask = True\n",
    "if store_pic_and_mask:\n",
    "    for key, value in new_tensors.items():\n",
    "        # Get stored values from first run:\n",
    "        input_tensor, original_prediction, label, mask = value\n",
    "        number = key\n",
    "        input_tensor = input_tensor.type(torch.float)\n",
    "\n",
    "        # Get original image with index and reshape(for plotting)\n",
    "        (img, label) = dataset_original[number]\n",
    "\n",
    "        mask_img = mask_input_image(img, mask)\n",
    "\n",
    "        dataset_original[key][0].save(f'image{key}.png')\n",
    "        im = Image.fromarray((np.reshape(mask, (300,300)) * 255).astype(np.uint8))\n",
    "        im.save(f'mask{key}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c464eb",
   "metadata": {},
   "source": [
    "## Rerun and calculate metrics\n",
    "\n",
    "Run model again with the masked image and calculate metrics based on the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96753f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in new_tensors.items():\n",
    "    # Get stored values from first run:\n",
    "    input_tensor, original_prediction, label, mask = value\n",
    "    number = key\n",
    "    input_tensor = input_tensor.type(torch.float)\n",
    "    \n",
    "    # Run the model\n",
    "    output = model(input_tensor)\n",
    "    output = torch.argmax(output)\n",
    "\n",
    "    # Get original image with index and reshape(for plotting)\n",
    "    (img, label) = dataset_original[number]\n",
    "    numpy_img = cv2.resize(np.array(img), (300, 300))\n",
    "    numpy_img = np.float32(img) / 255 # Assume 8 bit pixels\n",
    "\n",
    "    # Set target as our predicted label. TODO: Maybe target as the same label as the previous prediction?\n",
    "    targets = [ClassifierOutputTarget(original_prediction)]\n",
    "    # Define target layer\n",
    "    target_layers = [model[7][2]]\n",
    "\n",
    "    # Run model cams\n",
    "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_grad_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "        grad_cam_image = show_cam_on_image(numpy_img, grayscale_grad_cams[0, :], use_rgb=True)\n",
    "\n",
    "    with HiResCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_hires_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "        hires_cam_image = show_cam_on_image(numpy_img, grayscale_hires_cams[0, :], use_rgb=True)\n",
    "\n",
    "    img = mask_input_image(img, mask)\n",
    "    if plotting:\n",
    "        # Make images the same format and plot original, greyscale and heatmap:\n",
    "        grad_cam = np.uint8(255*grayscale_grad_cams[0, :])\n",
    "        grad_cam = cv2.merge([grad_cam, grad_cam, grad_cam])\n",
    "\n",
    "        hires_cam = np.uint8(255*grayscale_hires_cams[0, :])\n",
    "        hires_cam = cv2.merge([hires_cam, hires_cam, hires_cam])\n",
    "\n",
    "        images = np.hstack((img, grad_cam, grad_cam_image, hires_cam, hires_cam_image))\n",
    "        Image.fromarray(images).show()\n",
    "    print(output, \"  \", original_prediction, \"  \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98168bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediaEnv",
   "language": "python",
   "name": "mediaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
