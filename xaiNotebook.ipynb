{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e18888a",
   "metadata": {},
   "source": [
    "# XAI notebook\n",
    "Notebook defines way to test different CNN explainability techniques. This metric used masking and GAN to change the background of the given object in (classification) task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from utils import map_from_list\n",
    "from torchvision.datasets import VOCDetection, Caltech101, VOCSegmentation, ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ff70c",
   "metadata": {},
   "source": [
    "## Load datasets and pretrained networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f707cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose model\n",
    "model_name = 'ResNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665118cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ResNet':\n",
    "    # Import only if model used\n",
    "    from torchvision.models import resnet34, ResNet34_Weights\n",
    "    \n",
    "    # Loads best possible pre-trained weights for ImageNet dataset (further traning needed for other datasets)\n",
    "    weights = ResNet34_Weights.DEFAULT\n",
    "    # Init model with weights\n",
    "    model = resnet34(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'DenseNet':\n",
    "    # Import only if model used\n",
    "    from torchvision.models import densenet121, DenseNet121_Weights\n",
    "\n",
    "    # Loads best possible pre-trained weights for ImageNet dataset (further traning needed for other datasets)\n",
    "    weights = DenseNet121_Weights.DEFAULT\n",
    "    # Init model with weights\n",
    "    model = densenet121(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f59fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'C:/Users/pette/Documents/jupterNotebooks/machinelearning/datasets' # Own data root directory here\n",
    "choose_dataset = 'VOC' # VOC, Caltech, ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms for resnet and densenet. Output transform only needed for segmantation:\n",
    "def grey_scale_to_rgb(x):\n",
    "    if x.size(dim=0) == 3:\n",
    "        return x\n",
    "    else:\n",
    "        return x.repeat(3, 1, 1)\n",
    "    \n",
    "def VOC_to_label(x):\n",
    "    all_classes = ['horse', 'person', 'bottle', 'dog', 'tvmonitor', 'car', 'aeroplane', 'bicycle',\n",
    "                   'boat', 'chair', 'diningtable', 'pottedplant', 'train', 'cat', 'sofa', 'bird',\n",
    "                   'sheep', 'motorbike', 'bus', 'cow']\n",
    "    final_labels = torch.zeros(len(all_classes))\n",
    "    for one_object in x['annotation']['object']:\n",
    "        final_labels[all_classes.index(one_object['name'])] = 1\n",
    "    return final_labels\n",
    "\n",
    "transform_input = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(grey_scale_to_rgb),\n",
    "    transforms.Normalize(\n",
    "         mean=[0.485, 0.456, 0.406],\n",
    "         std=[0.229, 0.224, 0.225]\n",
    " )\n",
    "])\n",
    "transform_output = transforms.Compose([\n",
    "    transforms.Lambda(VOC_to_label)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if choose_dataset == 'VOC':\n",
    "    num_of_classes = 20\n",
    "    dataset = VOCDetection(root, year='2012', image_set='train', download=True, transform=transform_input, target_transform=transform_output)\n",
    "    dataset_original = VOCDetection(root, year='2012', image_set='train', download=True, transform=None, target_transform=None)\n",
    "    #dataset_segmentation = VOCSegmentation(root, year='2012', image_set='train', download=True, transform=None, target_transform=transform_output)\n",
    "elif choose_dataset == 'Caltech':\n",
    "    num_of_classes = 101\n",
    "    dataset = Caltech101(root, download=True, transform=transform_input, target_transform=None)\n",
    "    dataset_original = Caltech101(root, download=True, transform=None, target_transform=None)\n",
    "elif choose_dataset == 'ImageNet':\n",
    "    num_of_classes = 1000\n",
    "    root += '/imagenet_images'\n",
    "    dataset = ImageFolder(root, transform=transform_input, target_transform=None)\n",
    "    dataset_original = ImageFolder(root, transform=None, target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4706ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "batch_size = 4\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bdc51",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Model can be further trained with loaded datasets. All classification models pretrained weights are trained on Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "load_weights = False\n",
    "# Create optimizer and loss function\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "momentum = 0\n",
    "lr = 0.001\n",
    "\n",
    "if choose_dataset == 'VOC':\n",
    "    crit = torch.nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    crit = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "\n",
    "# Define number of epochs used for further training\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    model.fc = torch.nn.Linear(512, num_of_classes)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "                inputs, labels = data[0], data[1]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = crit(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                print(f\"{i}/{len(data_loader)}\")\n",
    "        torch.save(model.state_dict(), f'weights/latest{choose_dataset}.pth')\n",
    "        print(f\"Loss in epoch {epoch}: {running_loss/(len(data_loader)*batch_size)}\")\n",
    "else:\n",
    "    if load_weights:\n",
    "        model.load_state_dict(torch.load(f'weights/latest{choose_dataset}.pth'))\n",
    "    else:\n",
    "        pass # Use pretrained weights for XAI method evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64631f0",
   "metadata": {},
   "source": [
    "# GradCAM example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665f243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 1000\n",
    "# Get transformed tensor with index\n",
    "(input_tensor, labels) = dataset[i]\n",
    "\n",
    "\n",
    "# Get original image with index and reshape(for plotting)\n",
    "(img, label) = dataset_original[i]\n",
    "img = cv2.resize(np.array(img), (300, 300))\n",
    "img = np.float32(img) / 255 # Assume 8 bit pixels\n",
    "\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "if choose_dataset == 'VOC':\n",
    "    labels = (labels==1).nonzero().squeeze().tolist()\n",
    "\n",
    "if isinstance(labels, int):\n",
    "    labels = [labels]\n",
    "\n",
    "for label in labels:\n",
    "    print(labels)\n",
    "    if choose_dataset == 'ImageNet':\n",
    "        label, name = map_from_list(label, dataset.find_classes(root)[0])\n",
    "    # Set target as our ground truth label\n",
    "    targets = [ClassifierOutputTarget(label)]\n",
    "    # Define target layer\n",
    "    target_layers = [model.layer4]\n",
    "\n",
    "    # Run model with given cam\n",
    "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "    # Make images the same format and plot original, greyscale and heatmap:\n",
    "    cam = np.uint8(255*grayscale_cams[0, :])\n",
    "    cam = cv2.merge([cam, cam, cam])\n",
    "    images = np.hstack((np.uint8(255*img), cam , cam_image))\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c464eb",
   "metadata": {},
   "source": [
    "## Threshold and create a new image\n",
    "\n",
    "Threshold gradcam probabilities and multiply with the input tensor to get a new image to feed to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96753f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grayscale_cams)\n",
    "threshold = 0.3\n",
    "mask = (grayscale_cams > threshold).astype(int)\n",
    "\n",
    "masked_tensor = torch.mul(input_tensor, torch.tensor(mask))\n",
    "masked_image = np.uint8(255*img)*np.reshape(mask, (300,300,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4581bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masked_image, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd31e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediaEnv",
   "language": "python",
   "name": "mediaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
