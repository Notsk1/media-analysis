{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e18888a",
   "metadata": {},
   "source": [
    "# XAI notebook\n",
    "Notebook defines way to test different CNN explainability techniques. This metric used masking and GAN to change the background of the given object in (classification) task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.datasets import VOCSegmentation, CIFAR10, Caltech101\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306a938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ff70c",
   "metadata": {},
   "source": [
    "## Load datasets and pretrained networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f707cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "# Choose model\n",
    "model_name = 'ResNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665118cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'ResNet':\n",
    "    # Import only if model used\n",
    "    from torchvision.models import resnet34, ResNet34_Weights\n",
    "    \n",
    "    # Loads best possible pre-trained weights for ImageNet dataset (further traning needed for other datasets)\n",
    "    weights = ResNet34_Weights.DEFAULT\n",
    "    # Init model with weights\n",
    "    model = resnet34(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3c962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'DenseNet':\n",
    "    # Import only if model used\n",
    "    from torchvision.models import densenet121, DenseNet121_Weights\n",
    "\n",
    "    # Loads best possible pre-trained weights for ImageNet dataset (further traning needed for other datasets)\n",
    "    weights = DenseNet121_Weights.DEFAULT\n",
    "    # Init model with weights\n",
    "    model = densenet121(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f59fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'C:/Users/pette/Documents/jupterNotebooks/machinelearning/datasets' # Own data root directory here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83bf02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms for resnet and densenet. Output transform only needed for segmantation:\n",
    "transform_input = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "         mean=[0.485, 0.456, 0.406],\n",
    "         std=[0.229, 0.224, 0.225]\n",
    " )\n",
    "])\n",
    "transform_output = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2c3f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(<PIL.Image.Image image mode=RGB size=32x32 at 0x2D3C85648E0>, 4)\n",
      "(tensor([[[-1.2103, -1.2103, -1.2103,  ..., -1.7069, -1.7069, -1.7069],\n",
      "         [-1.2103, -1.2103, -1.2103,  ..., -1.7069, -1.7069, -1.7069],\n",
      "         [-1.2103, -1.2103, -1.2103,  ..., -1.7069, -1.7069, -1.7069],\n",
      "         ...,\n",
      "         [-0.6623, -0.6623, -0.6623,  ..., -1.2788, -1.2788, -1.2788],\n",
      "         [-0.6623, -0.6623, -0.6623,  ..., -1.2788, -1.2788, -1.2788],\n",
      "         [-0.6623, -0.6623, -0.6623,  ..., -1.2788, -1.2788, -1.2788]],\n",
      "\n",
      "        [[-0.8978, -0.8978, -0.8978,  ..., -1.5455, -1.5455, -1.5455],\n",
      "         [-0.8978, -0.8978, -0.8978,  ..., -1.5455, -1.5455, -1.5455],\n",
      "         [-0.8978, -0.8978, -0.8978,  ..., -1.5455, -1.5455, -1.5455],\n",
      "         ...,\n",
      "         [-0.5826, -0.5826, -0.5826,  ..., -1.1604, -1.1604, -1.1604],\n",
      "         [-0.5826, -0.5826, -0.5826,  ..., -1.1604, -1.1604, -1.1604],\n",
      "         [-0.5826, -0.5826, -0.5826,  ..., -1.1604, -1.1604, -1.1604]],\n",
      "\n",
      "        [[-0.8807, -0.8807, -0.8807,  ..., -1.3339, -1.3339, -1.3339],\n",
      "         [-0.8807, -0.8807, -0.8807,  ..., -1.3339, -1.3339, -1.3339],\n",
      "         [-0.8807, -0.8807, -0.8807,  ..., -1.3339, -1.3339, -1.3339],\n",
      "         ...,\n",
      "         [-0.5670, -0.5670, -0.5670,  ..., -1.0898, -1.0898, -1.0898],\n",
      "         [-0.5670, -0.5670, -0.5670,  ..., -1.0898, -1.0898, -1.0898],\n",
      "         [-0.5670, -0.5670, -0.5670,  ..., -1.0898, -1.0898, -1.0898]]]), 4)\n"
     ]
    }
   ],
   "source": [
    "#voc_dataset = VOCSegmentation(root, year='2012', image_set='train', download=True, transform=transform_input, target_transform=transform_output)\n",
    "#voc_original_dataset = VOCSegmentation(root, year='2012', image_set='train', download=True, transform=None, target_transform=None)\n",
    "#dataset = Caltech101(root, download=True, transform=transform_input, target_transform=None)\n",
    "#dataset_original = Caltech101(root, download=True, transform=None, target_transform=None)\n",
    "dataset = CIFAR10(root, train=True, download=True, transform=transform_input, target_transform=None)\n",
    "dataset_original = CIFAR10(root, train=True, download=True, transform=None, target_transform=None)\n",
    "print(dataset_original[10])\n",
    "print(dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4706ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bdc51",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Model can be further trained with loaded datasets. All classification models pretrained weights are trained on Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d14611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibly add support for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "203026ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 2.1975,  2.1975,  2.1975,  ...,  2.1804,  2.1804,  2.1804],\n",
      "         [ 2.1975,  2.1975,  2.1975,  ...,  2.1804,  2.1804,  2.1804],\n",
      "         [ 2.1975,  2.1975,  2.1975,  ...,  2.1804,  2.1804,  2.1804],\n",
      "         ...,\n",
      "         [ 1.9235,  1.9235,  1.9235,  ...,  2.1804,  2.1804,  2.1804],\n",
      "         [ 1.9235,  1.9235,  1.9235,  ...,  2.1804,  2.1804,  2.1804],\n",
      "         [ 1.9235,  1.9235,  1.9235,  ...,  2.1804,  2.1804,  2.1804]],\n",
      "\n",
      "        [[-1.8957, -1.8957, -1.8957,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         [-1.8957, -1.8957, -1.8957,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         [-1.8957, -1.8957, -1.8957,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         ...,\n",
      "         [-1.3880, -1.3880, -1.3880,  ..., -1.0203, -1.0203, -1.0203],\n",
      "         [-1.3880, -1.3880, -1.3880,  ..., -1.0203, -1.0203, -1.0203],\n",
      "         [-1.3880, -1.3880, -1.3880,  ..., -1.0203, -1.0203, -1.0203]],\n",
      "\n",
      "        [[-1.0724, -1.0724, -1.0724,  ..., -1.2816, -1.2816, -1.2816],\n",
      "         [-1.0724, -1.0724, -1.0724,  ..., -1.2816, -1.2816, -1.2816],\n",
      "         [-1.0724, -1.0724, -1.0724,  ..., -1.2816, -1.2816, -1.2816],\n",
      "         ...,\n",
      "         [-0.6193, -0.6193, -0.6193,  ..., -0.2707, -0.2707, -0.2707],\n",
      "         [-0.6193, -0.6193, -0.6193,  ..., -0.2707, -0.2707, -0.2707],\n",
      "         [-0.6193, -0.6193, -0.6193,  ..., -0.2707, -0.2707, -0.2707]]]), 3)\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer and loss function\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "momentum = 0\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "\n",
    "# Define number of epochs used for further training\n",
    "epochs = 2\n",
    "print(dataset[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "            inputs, labels = data[0], data[1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = crit(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print(i)\n",
    "    print(f\"Loss in epoch {epoch}: {running_loss/(len(loader)*batch_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64631f0",
   "metadata": {},
   "source": [
    "# GradCAM example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transformed tensor with index\n",
    "(input_tensor, label) = dataset[10]\n",
    "# Get original image with index and reshape(for plotting)\n",
    "(img, label) = dataset_original[10]\n",
    "img = cv2.resize(np.array(img), (300, 300))\n",
    "img = np.float32(img) / 255 # Assume 8 bit pixels\n",
    "\n",
    "\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "# Set target as our ground truth label\n",
    "targets = [ClassifierOutputTarget(label)]\n",
    "# Define target layer\n",
    "target_layers = [model.layer4]\n",
    "\n",
    "# Run model with given cam\n",
    "with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "\n",
    "# Make images the same format and plot original, greyscale and heatmap:\n",
    "cam = np.uint8(255*grayscale_cams[0, :])\n",
    "cam = cv2.merge([cam, cam, cam])\n",
    "images = np.hstack((np.uint8(255*img), cam , cam_image))\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(data_loader):\n",
    "    image, segmentation = data\n",
    "    prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73248c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediaEnv",
   "language": "python",
   "name": "mediaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
